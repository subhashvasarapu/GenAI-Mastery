# Safety and Limitations
LLMs can hallucinate facts. Mitigation includes RLHF, moderation filters, and disclaimers.