# Transformer Model Explained

The transformer architecture enables parallel processing of data with high accuracy.

## Self-Attention:
It computes how each word relates to every other word in a sentence to maintain context.

## Components:
- **Multi-Head Attention**: Captures different relationships in the data.
- **Feedforward Network**: Processes the output from attention layers.
- **Positional Encoding**: Maintains word order.

## Example:
In "The cat sat on the mat", 'sat' relates more to 'cat' than 'mat'â€”transformer captures this.
