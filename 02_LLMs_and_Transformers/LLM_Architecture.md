# Large Language Model (LLM) Architecture

LLMs are built using transformer-based deep neural networks trained on massive text corpora.

## Components:
- **Tokenizer**: Converts words to numerical tokens.
- **Embedding Layer**: Maps tokens into vectors.
- **Transformer Blocks**: Handle contextual relationships using self-attention.
- **Output Layer**: Decodes predictions into natural language.

## Example:
- GPT-3 has 175 billion parameters and uses autoregressive prediction.
- BERT is bidirectional and useful for classification and question answering tasks.
